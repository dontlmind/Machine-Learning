# -*- coding: utf-8 -*-
"""Image Classification (CNN)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15anpw-NYqolPffPAzmGVPMeHfnIVCODU

## Task Description

Solve image classification with convolutional neural networks.

The images are collected from the food-11 dataset classified into 11
classes.

Training set: 280 * 11 labeled images + 6786 unlabeled images.

Validation set: 60 * 11 labeled images.

Testing set: 3347 images.

##  Packages
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset
from torchvision.datasets import DatasetFolder
from torchvision import transforms
import os
import matplotlib.pyplot as plt
import torchvision.models as models
from PIL import Image
from tqdm import tqdm
import seaborn as sns
from torch.optim.lr_scheduler import CosineAnnealingLR

myseed = 12345  # set seed for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(myseed)
torch.manual_seed(myseed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(myseed)

"""## Import Data
Food images of 11 classes
- Training set: 280 * 11 labeled images + 6786 unlabeled images
- Validation set: 60 * 11 labeled images
- Testing set: 3347 images

"""

# download dataset
!sudo apt install megatools
!megadl "https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg"

# unzip dataset
import zipfile
def un_zipFiles(path):
    files=os.listdir(path)
    for file in files:
        if file.endswith('.zip'):
            filePath=path+'/'+file
            zip_file = zipfile.ZipFile(filePath)
            for names in zip_file.namelist():
                zip_file.extract(names,path)
            zip_file.close() 
un_zipFiles('../content/')

"""## Load Dataset / Image Augmentation


"""

train_tfm = transforms.Compose([
    # random crop
    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),
    # random horizontal flip, prob = 0.5
    transforms.RandomHorizontalFlip(),
    # random noise
    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4,hue=0.1),
    transforms.ToTensor(),
    # normalize
    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])
test_tfm = transforms.Compose([
    # resize image to 256*256, then center crop to 224*224
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    # normalize
    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])

# Define batch size for training, validation, and testing.
batch_size = 32

# Construct datasets.
train_set = DatasetFolder("food-11/training/labeled", loader=lambda x: Image.open(x), extensions="jpg", transform=train_tfm)
valid_set = DatasetFolder("food-11/validation", loader=lambda x: Image.open(x), extensions="jpg", transform=test_tfm)
unlabeled_set = DatasetFolder("food-11/training/unlabeled", loader=lambda x: Image.open(x), extensions="jpg", transform=train_tfm)
test_set = DatasetFolder("food-11/testing", loader=lambda x: Image.open(x), extensions="jpg", transform=test_tfm)

# Construct data loaders.
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)

"""## Image Example """

# Show some original images

def no_axis_show(img, title='', cmap=None):
  # imshow, and set the interpolation mode to be "nearest"。
  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)
  # do not show the axes in the images.
  fig.axes.get_xaxis().set_visible(False)
  fig.axes.get_yaxis().set_visible(False)
  plt.title(title)

plt.figure(figsize=(18, 18))

for i in range(10):
  plt.subplot(1, 10, i+1)
  fig = no_axis_show(plt.imread(f'food-11/training/labeled/0{i}/{i}_0.jpg'), title=i)

# Sample transformed Images

plt.figure(figsize=(18, 18))

sample_tfm = transforms.Compose([
    # random crop
    transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)),
    # random horizontal flip, prob = 0.5
    transforms.RandomHorizontalFlip(),
    # random noise
    transforms.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.2,hue=0.1),
    transforms.ToTensor(),

    ])

for i in range(10):
  img = Image.open(f'food-11/training/labeled/0{i}/{i}_0.jpg')
  img = sample_tfm(img)
  img = img.T
  plt.subplot(2, 10, i+1)
  fig = no_axis_show(img, title=i)

"""## **Model**

resnext50 from pytorch vision
"""

def model(num_classes, use_pretrained=True):
    """
    Load model from pytorch.

    Args:
        num_classes: number of classification classes.
        use_pretrained: load pretrained parameters.
    """
    model_ft = models.resnext50_32x4d(pretrained=use_pretrained)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, num_classes)
    nn.init.xavier_uniform_(model_ft.fc.weight)

    return model_ft

"""## **Training**

### Semi-superviced Learning
"""

class PseudoDataset(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.y)

    def __getitem__(self, id):
        return self.x[id][0], self.y[id]

def get_pseudo_labels(dataset, model, threshold=0.9):
    """
    Get pseudo labels for unlabeled data based on model inference.

    Args: 
        dataset: unlabeled dataset.
        model: inference model.
        threshold: softmax threshold, datas only get pseudo_labels
              when class probability is greater than threshold.
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"

    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    model.eval()
    softmax = nn.Softmax(dim=-1)

    idx = []
    labels = []

    for i, batch in enumerate(tqdm(data_loader)):
        img, _ = batch
        with torch.no_grad():
            logits = model(img.to(device))
        probs = softmax(logits)

        for j, x in enumerate(probs):
            if torch.max(x) > threshold:
                idx.append(i * batch_size + j)
                labels.append(int(torch.argmax(x)))

    model.train()
    print ("\nNew data: {:5d}\n".format(len(idx)))
    dataset = PseudoDataset(Subset(dataset, idx), labels)
    return dataset

"""### Parameters and model"""

# "cuda" only when GPUs are available.
device = "cuda" if torch.cuda.is_available() else "cpu"

# Initialize a model, and put it on the device specified.
model = model(11).to(device)
model.device = device

# classification task criterion, measurement of performance.
criterion = nn.CrossEntropyLoss()

# Initialize optimizer and cosine learning curve.
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
scheduler = CosineAnnealingLR(optimizer,T_max=10)

# The number of training epochs.
n_epochs = 20

# Model path
model_path = '../content'

# Whether to do semi-supervised learning.
do_semi = True
threshold = 0.9

"""### Train"""

# Record bset accuracy in Dev set.
bst_acc = 0

# Iterate epochs.
for epoch in range(n_epochs):
    # Inference and combine pseudo dataset with labeled dataset
    if do_semi:
        # Obtain pseudo-labels for unlabeled data using trained model.
        pseudo_set = get_pseudo_labels(unlabeled_set, model, threshold)
        # Construct a new dataset and a data loader for training.
        concat_dataset = ConcatDataset([train_set, pseudo_set])
        train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)

    # ---------- Training ----------
    # Make sure the model is in train mode before training.
    model.train()

    # Record information in training.
    train_loss = []
    train_accs = []

    # Iterate the training set by batches.
    for batch in tqdm(train_loader):

        imgs, labels = batch
        logits = model(imgs.to(device))
        loss = criterion(logits, labels.to(device))
        optimizer.zero_grad()
        loss.backward()

        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)

        optimizer.step()

        # Compute the accuracy for current batch.
        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        # Record the loss and accuracy.
        train_loss.append(loss.item())
        train_accs.append(acc)

    print("Epoch %d learning rate：%f" % (epoch+1,optimizer.param_groups[0]['lr']))
    scheduler.step()
    
    train_loss = sum(train_loss) / len(train_loss)
    train_acc = sum(train_accs) / len(train_accs)

    # Print the information.
    print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

    # ---------- Validation ----------
    # Make sure the model is in evaluation mode before training.
    model.eval()

    # Record information in validation.
    valid_loss = []
    valid_accs = []

    # Iterate the validation set by batches.
    for batch in tqdm(valid_loader):

        imgs, labels = batch

        with torch.no_grad():
          logits = model(imgs.to(device))

        
        loss = criterion(logits, labels.to(device))

        # Compute the accuracy for current batch.
        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        # Record the loss and accuracy.
        valid_loss.append(loss.item())
        valid_accs.append(acc)

    
    valid_loss = sum(valid_loss) / len(valid_loss)
    valid_acc = sum(valid_accs) / len(valid_accs)

    # Print the information.
    print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")

    # Save best model. 
    if valid_acc > bst_acc:
      bst_acc = valid_acc
      torch.save(model.state_dict(),f'{model_path}/model.pth')
      print(f'Save model with accuracy {bst_acc}')

"""## Inference


"""

# Load best model.
model.load_state_dict(torch.load(f'{model_path}/model.pth'))

model.eval()

# Initialize a list to store the predictions.
predictions = []

# Iterate the testing set by batches.
for batch in tqdm(test_loader):

    imgs, labels = batch

    with torch.no_grad():
        logits = model(imgs.to(device))

    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())

# result path
result_path = '../content'

# Save predictions into the file.
result = pd.DataFrame()
result['Id'] = range(0,len(test_set))
result['Category'] = predictions
result.to_csv( f'{result_path}/result.csv',index = False)